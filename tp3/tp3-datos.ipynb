{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parte 1 – Conjunto de Datos","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.utils.random import sample_without_replacement\nimport os\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"darkgrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"TSoB1eVp03L-","execution":{"iopub.status.busy":"2022-08-17T22:06:34.583834Z","iopub.execute_input":"2022-08-17T22:06:34.585771Z","iopub.status.idle":"2022-08-17T22:06:35.923799Z","shell.execute_reply.started":"2022-08-17T22:06:34.585626Z","shell.execute_reply":"2022-08-17T22:06:35.922756Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA_PATH = \"/kaggle/input/amex-default-prediction/train_data.csv\"\nTRAIN_LABELS_PATH = \"/kaggle/input/amex-default-prediction/train_labels.csv\"\nTEST_DATA_PATH = \"/kaggle/input/amex-default-prediction/test_data.csv\"\nSAMPLE_SUBMISSION_PATH = \"/kaggle/input/amex-default-prediction/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:35.925605Z","iopub.execute_input":"2022-08-17T22:06:35.926093Z","iopub.status.idle":"2022-08-17T22:06:35.931996Z","shell.execute_reply.started":"2022-08-17T22:06:35.926017Z","shell.execute_reply":"2022-08-17T22:06:35.931333Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Dado que el archivo **train_data.csv** es muy grande y que la plataforma del notebook ofrece una memoria limitada, vamos a leer el archivo en chunks de tamaño 100.000 .\n\nLuego, para el sampleo utilizaremos el archivo **train_labels.csv** el cual es considerablemente pequeño y contiene el ID de cada cliente junto con su target.\n\nLuego guardamos en un dataframe los IDs de los clientes que vamos a utilizar.","metadata":{}},{"cell_type":"code","source":"df_train_total = pd.read_csv(TRAIN_DATA_PATH, chunksize=100000)\ndf_train_labels = pd.read_csv(TRAIN_LABELS_PATH)\n\nnumero_grupo = 2\nfilas = len(df_train_labels)\nsemilla = (31416 * numero_grupo)%1000\nn_population = filas # cantidad de casos totales\nn_samples = filas*0.05 # cantidad de casos que vamos a utilizar\n\nsample = sample_without_replacement(random_state = semilla,\n                                    n_population = n_population,\n                                    n_samples = n_samples)\nsample.sort()\n\ndf_sample_casos = df_train_labels.iloc[sample]","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:35.933222Z","iopub.execute_input":"2022-08-17T22:06:35.933821Z","iopub.status.idle":"2022-08-17T22:06:37.122253Z","shell.execute_reply.started":"2022-08-17T22:06:35.933785Z","shell.execute_reply":"2022-08-17T22:06:37.121081Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Una vez obtenidos los casos que vamos a utilizar, realizamos un inner join del dataframe con los IDs de los clientes y los chunks del archivo **train_data.csv**. Luego guardamos en un archivo csv el dataframe final.","metadata":{}},{"cell_type":"code","source":"def merge_data(data_to_use, total_data):\n    df_train = pd.DataFrame()\n    for chunk in total_data:\n        data = pd.merge(data_to_use, chunk, left_on='customer_ID', right_on='customer_ID')\n        df_train = df_train.append(data, ignore_index=True)\n        del data\n        del chunk\n    return df_train","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:37.127008Z","iopub.execute_input":"2022-08-17T22:06:37.127458Z","iopub.status.idle":"2022-08-17T22:06:37.136550Z","shell.execute_reply.started":"2022-08-17T22:06:37.127422Z","shell.execute_reply":"2022-08-17T22:06:37.135382Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Debido a que el iterar por todos los chunks del dataframe tarda mucho, decidimos hacerlo una unica vez y simplemente cargar el dataframe a utilizar en un csv y cargarlo directamente con pandas.\n> Warning: La siguiente celda tarda bastante","metadata":{}},{"cell_type":"code","source":"'''\ndf = merge_data(df_sample_casos, df_train_total)\ndf.to_csv('train_final.csv', sep=\";\")'''","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:37.138295Z","iopub.execute_input":"2022-08-17T22:06:37.139011Z","iopub.status.idle":"2022-08-17T22:06:37.153021Z","shell.execute_reply.started":"2022-08-17T22:06:37.138963Z","shell.execute_reply":"2022-08-17T22:06:37.151817Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Una vez tenemos el csv de train final, lo utilizamos para crear nuestro dataframe de entrenamiento.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"https://drive.google.com/uc?export=download&id=171ytMl9FD_3ykCZwrZbI0iXlrZKuRYaS&confirm=t&uuid=4d6c6d66-735a-4be6-baca-fb65f980bcd8\", sep=\";\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:37.154242Z","iopub.execute_input":"2022-08-17T22:06:37.154566Z","iopub.status.idle":"2022-08-17T22:06:56.066112Z","shell.execute_reply.started":"2022-08-17T22:06:37.154537Z","shell.execute_reply":"2022-08-17T22:06:56.064595Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Parte 2 – Ciencia de Datos","metadata":{}},{"cell_type":"markdown","source":"## Análisis y exploración inicial","metadata":{}},{"cell_type":"markdown","source":"Empezamos viendo como quedo nuestro dataframe de entrenamiento","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:56.067782Z","iopub.execute_input":"2022-08-17T22:06:56.068619Z","iopub.status.idle":"2022-08-17T22:06:56.165712Z","shell.execute_reply.started":"2022-08-17T22:06:56.068581Z","shell.execute_reply":"2022-08-17T22:06:56.163723Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Como vemos hay una columna no deseada llamada _Unnamed: 0_ es probable que sea producto de un error en el merge hecho anteriormente o el pasar el dataframe a un nuevo csv asi que vamos a borrarla","metadata":{}},{"cell_type":"code","source":"df_train.drop(columns = [\"Unnamed: 0\"], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:56.166964Z","iopub.execute_input":"2022-08-17T22:06:56.167371Z","iopub.status.idle":"2022-08-17T22:06:56.450106Z","shell.execute_reply.started":"2022-08-17T22:06:56.167337Z","shell.execute_reply":"2022-08-17T22:06:56.447095Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Vamos a imprimir un poco de info para ver como estan compuestos nuestros datos","metadata":{}},{"cell_type":"code","source":"df_train.info(verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:56.454011Z","iopub.execute_input":"2022-08-17T22:06:56.455290Z","iopub.status.idle":"2022-08-17T22:06:56.559811Z","shell.execute_reply.started":"2022-08-17T22:06:56.455173Z","shell.execute_reply":"2022-08-17T22:06:56.554446Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Vamos a ver como estan distribuidos nuestros valores en la variable target","metadata":{}},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(9, 5)})\nax = sns.countplot(x=\"target\", data=df_train)\nax.set_xticklabels([\"Non-Default\", \"Default\"])\nplt.title(\"Cantidad de casos segun target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:56.569272Z","iopub.execute_input":"2022-08-17T22:06:56.570688Z","iopub.status.idle":"2022-08-17T22:06:56.855479Z","shell.execute_reply.started":"2022-08-17T22:06:56.570559Z","shell.execute_reply":"2022-08-17T22:06:56.854198Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Con un analisis inicial vemos que tenemos un dataset de entrenamiento algo desbalanceado, pero no parece lo suficiente como para intentar utilizar técnicas de balanceo en una primera instancia, de todos modos queda como una opción a considerar según los resultados que obtengamos con los modelos","metadata":{}},{"cell_type":"markdown","source":"### Sobre los datos faltantes o mal ingresados\n\nAnalizamos la existencia de NaNs en cada columna de nuestro dataset","metadata":{}},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(9, 5)})\ndf_train.isnull().any().value_counts().plot(kind='barh')\nplt.title(\"Cantidad de features segun contengan o no valores nulos\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:56.857313Z","iopub.execute_input":"2022-08-17T22:06:56.857839Z","iopub.status.idle":"2022-08-17T22:06:57.153200Z","shell.execute_reply.started":"2022-08-17T22:06:56.857793Z","shell.execute_reply":"2022-08-17T22:06:57.151895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Observamos entonces que estamos trabajando con nans en más de la mitad de nuestras columnas, ahora queremos saber la proporción de los mismos en cada una.","metadata":{}},{"cell_type":"code","source":"background_color = 'white'\nmissing = pd.DataFrame(columns = ['% Missing Data'],data = df_train.isna().sum()/len(df_train))\nmissing = missing.sort_values(by=['% Missing Data'])\nfig = plt.figure(figsize = (20, 60),facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.5, hspace = 0.5)\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\nsns.heatmap(missing.loc[missing['% Missing Data'] > 0],cbar = False,annot = True,fmt =\".2%\", linewidths = 2,vmax = 1, ax = ax0)\nplt.title(\"Porcentaje de NaNs por feature\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:57.154785Z","iopub.execute_input":"2022-08-17T22:06:57.155884Z","iopub.status.idle":"2022-08-17T22:06:59.482197Z","shell.execute_reply.started":"2022-08-17T22:06:57.155841Z","shell.execute_reply":"2022-08-17T22:06:59.480011Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Hay muchas columnas con más de un 80% de nans, consideramos que estas columnas son fuertes candidatas a ser eliminadas ya que tienen su gran mayoria de datos faltantes y va a ser dificil realizar alguna tecnica de sustitucion para esas columnas ya que podrian ser poco representativas. Quizás para las que tienen menor porcentaje de nans se puede recurrir a alguna técnica para trabajar con los mismos.","metadata":{}},{"cell_type":"markdown","source":"Una suposicion que quizas se podría tener en cuenta en un análisis más profundo es que las columnas con D (Delinquency) tienen grandes cantidades de NaNs en sus valores y puede deberse a que si el cliente no cometio ningun delito o algo que se relaciona con esto se llena con NaN ese valor.","metadata":{}},{"cell_type":"markdown","source":"### Sobre los valores atípicos","metadata":{}},{"cell_type":"markdown","source":"Como dice la consigna y la descripcion dada en la competencia del dataset, presentamos muchas variables categoricas que tienen valores numericos. Como no conocemos que son estos valores decidimos no mapearlos a ninguna categoria en particular y trabajar con ellos directamente.","metadata":{}},{"cell_type":"code","source":"categoricas = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64' ,'D_66', 'D_68']\nignorar = ['S_2', 'customer_ID', 'target']","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:59.484825Z","iopub.execute_input":"2022-08-17T22:06:59.485315Z","iopub.status.idle":"2022-08-17T22:06:59.491664Z","shell.execute_reply.started":"2022-08-17T22:06:59.485267Z","shell.execute_reply":"2022-08-17T22:06:59.490800Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Creamos una funcion que nos permita calcular el z_score en su version modificada para cada una de los valores del dataframe, asi podemos chequear por outliers.","metadata":{}},{"cell_type":"code","source":"def z_score_modificado(columna):\n    mediana = columna.median()\n    MAD = (columna - mediana).abs().median()\n    return ((columna - mediana)*0.6745)/MAD","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:59.492930Z","iopub.execute_input":"2022-08-17T22:06:59.493595Z","iopub.status.idle":"2022-08-17T22:06:59.509553Z","shell.execute_reply.started":"2022-08-17T22:06:59.493548Z","shell.execute_reply":"2022-08-17T22:06:59.508680Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"aux = {}\nfor column in df_train:\n    if column not in categoricas and column not in ignorar:\n        aux[column] = z_score_modificado(df_train[column])","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:06:59.511055Z","iopub.execute_input":"2022-08-17T22:06:59.511512Z","iopub.status.idle":"2022-08-17T22:07:01.960710Z","shell.execute_reply.started":"2022-08-17T22:06:59.511474Z","shell.execute_reply":"2022-08-17T22:07:01.959795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_aux = pd.DataFrame(aux)\ndf_aux","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:01.961981Z","iopub.execute_input":"2022-08-17T22:07:01.962367Z","iopub.status.idle":"2022-08-17T22:07:02.222260Z","shell.execute_reply.started":"2022-08-17T22:07:01.962332Z","shell.execute_reply":"2022-08-17T22:07:02.221281Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Ahora que ya tenemos el z-score modificado calculado para cada uno de los valores de nuestros features, vamos a visualizar que porcentaje de outliers tiene cada columna, considerando que con un z-score mayor a 3.5 seria considerado un outlier","metadata":{}},{"cell_type":"code","source":"background_color = 'white'\noutliers = pd.DataFrame(columns = ['% Outliers'],data = (df_aux > 3.5).sum()/len(df_aux))\noutliers = outliers.sort_values(by=['% Outliers'])\nfig = plt.figure(figsize = (20, 60),facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.5, hspace = 0.5)\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\nsns.heatmap(outliers.loc[outliers['% Outliers'] > 0],cbar = False,annot = True,fmt =\".4%\", linewidths = 2,vmax = 1, ax = ax0)\nplt.title(\"Porcentaje de outliers por feature (>3.5)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:02.223782Z","iopub.execute_input":"2022-08-17T22:07:02.224178Z","iopub.status.idle":"2022-08-17T22:07:05.095015Z","shell.execute_reply.started":"2022-08-17T22:07:02.224144Z","shell.execute_reply":"2022-08-17T22:07:05.093846Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Como vemos hay muchas columnas con una enorme cantidad de outliers segun z-score, de todas maneras consideramos que para el caso de estudio estos outliers nos pueden dar informacion muy valiosa ya que en este caso la clase dificil de predecir es justamente la que muestra casos positivos.","metadata":{}},{"cell_type":"markdown","source":"Para seguir un poco mas con el analisis de outliers decidimos dividir todos nuestros datos segun la categoria general asociada\n* D_* = Delinquency variables\n* S_* = Spend variables\n* P_* = Payment variables\n* B_* = Balance variables\n* R_* = Risk variables","metadata":{}},{"cell_type":"markdown","source":"Los siguientes graficos no son de mucha ayuda para visualizar valores concretos ya que tenemos muchas features para poder graficar, pero si nos sirven para detectar outliers severos o que se vayan mucho de los valores normales para cada columna","metadata":{}},{"cell_type":"code","source":"D_columns = [col for col in df_train if col.startswith('D')]\ndf_train[D_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:05.096549Z","iopub.execute_input":"2022-08-17T22:07:05.097088Z","iopub.status.idle":"2022-08-17T22:07:06.609781Z","shell.execute_reply.started":"2022-08-17T22:07:05.097014Z","shell.execute_reply":"2022-08-17T22:07:06.608636Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_with_d_columns = df_train[D_columns].copy()\nsns.set(rc={\"figure.figsize\":(25, 20)})\nfig, axes = plt.subplots(5, 2)\nsns.boxplot(data=df_with_d_columns.iloc[:, 0:9], ax=axes[0,0])\nsns.boxplot(data=df_with_d_columns.iloc[:, 10:19], ax=axes[0,1])\nsns.boxplot(data=df_with_d_columns.iloc[:, 20:29], ax=axes[1,0])\nsns.boxplot(data=df_with_d_columns.iloc[:, 30:39],ax=axes[1,1])\nsns.boxplot(data=df_with_d_columns.iloc[:, 40:49],ax=axes[2,0])\nsns.boxplot(data=df_with_d_columns.iloc[:, 50:59],ax=axes[2,1])\nsns.boxplot(data=df_with_d_columns.iloc[:, 60:69],ax=axes[3,0])\nsns.boxplot(data=df_with_d_columns.iloc[:, 70:79],ax=axes[3,1])\nsns.boxplot(data=df_with_d_columns.iloc[:, 80:89],ax=axes[4,0])\nsns.boxplot(data=df_with_d_columns.iloc[:, 90:96],ax=axes[4,1])\nplt.suptitle('Delinquency features', fontsize=20, y=0.93)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:06.611576Z","iopub.execute_input":"2022-08-17T22:07:06.611926Z","iopub.status.idle":"2022-08-17T22:07:12.155041Z","shell.execute_reply.started":"2022-08-17T22:07:06.611897Z","shell.execute_reply":"2022-08-17T22:07:12.154091Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"S_columns = [col for col in df_train if col.startswith('S')]\ndf_train[S_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:12.156205Z","iopub.execute_input":"2022-08-17T22:07:12.156539Z","iopub.status.idle":"2022-08-17T22:07:12.543847Z","shell.execute_reply.started":"2022-08-17T22:07:12.156508Z","shell.execute_reply":"2022-08-17T22:07:12.542846Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_with_s_columns = df_train[S_columns].copy()\nsns.set(rc={\"figure.figsize\":(15, 5)})\nax = sns.boxplot(data=df_with_s_columns[['S_5', 'S_12', 'S_16', 'S_22', 'S_23', 'S_24', 'S_26']])\nplt.title('Spend features')\nplt.show()\n\ndf_with_s_columns = df_train[S_columns].copy()\nsns.set(rc={\"figure.figsize\":(15, 5)})\nax = sns.boxplot(data=df_with_s_columns[['S_3', 'S_6', 'S_7', 'S_8', 'S_9', 'S_11', 'S_13', 'S_15', 'S_17', 'S_18', 'S_19', 'S_20', 'S_23', 'S_25', 'S_27']])\nplt.title('Spend features')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:12.545323Z","iopub.execute_input":"2022-08-17T22:07:12.545780Z","iopub.status.idle":"2022-08-17T22:07:14.715536Z","shell.execute_reply.started":"2022-08-17T22:07:12.545736Z","shell.execute_reply":"2022-08-17T22:07:14.714537Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"P_columns = [col for col in df_train if col.startswith('P')]\ndf_train[P_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:14.716585Z","iopub.execute_input":"2022-08-17T22:07:14.716939Z","iopub.status.idle":"2022-08-17T22:07:14.791641Z","shell.execute_reply.started":"2022-08-17T22:07:14.716907Z","shell.execute_reply":"2022-08-17T22:07:14.790938Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_with_p_columns = df_train[P_columns].copy()\nsns.set(rc={\"figure.figsize\":(10, 5)})\nax = sns.boxplot(data=df_with_p_columns)\nplt.title('Payment features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:14.793205Z","iopub.execute_input":"2022-08-17T22:07:14.793607Z","iopub.status.idle":"2022-08-17T22:07:15.115484Z","shell.execute_reply.started":"2022-08-17T22:07:14.793576Z","shell.execute_reply":"2022-08-17T22:07:15.114467Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"B_columns = [col for col in df_train if col.startswith('B')]\ndf_train[B_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:15.116839Z","iopub.execute_input":"2022-08-17T22:07:15.117250Z","iopub.status.idle":"2022-08-17T22:07:15.773743Z","shell.execute_reply.started":"2022-08-17T22:07:15.117213Z","shell.execute_reply":"2022-08-17T22:07:15.772630Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_with_b_columns = df_train[B_columns].copy()\nsns.set(rc={\"figure.figsize\":(25, 5)})\nax = sns.boxplot(data=df_with_b_columns)\nplt.title('Balance features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:15.775006Z","iopub.execute_input":"2022-08-17T22:07:15.775450Z","iopub.status.idle":"2022-08-17T22:07:18.507369Z","shell.execute_reply.started":"2022-08-17T22:07:15.775418Z","shell.execute_reply":"2022-08-17T22:07:18.506226Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"R_columns = [col for col in df_train if col.startswith('R')]\ndf_train[R_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:18.508582Z","iopub.execute_input":"2022-08-17T22:07:18.508981Z","iopub.status.idle":"2022-08-17T22:07:18.998011Z","shell.execute_reply.started":"2022-08-17T22:07:18.508947Z","shell.execute_reply":"2022-08-17T22:07:18.997083Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_with_r_columns = df_train[R_columns].copy()\nsns.set(rc={\"figure.figsize\":(25, 5)})\nax = sns.boxplot(data=df_with_r_columns)\nplt.title('Risk features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:18.999344Z","iopub.execute_input":"2022-08-17T22:07:18.999761Z","iopub.status.idle":"2022-08-17T22:07:20.463151Z","shell.execute_reply.started":"2022-08-17T22:07:18.999729Z","shell.execute_reply":"2022-08-17T22:07:20.462096Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Estas visualizaciones pueden ser de ayuda en caso de que mas adelante en la investigacion determinemos que una cierta feature tiene mucha correlacion o sirve mucho para predecir nuestro target, saber si tiene valores atipicos y actuar en consecuencia para mejorar nuestros modelos.","metadata":{}},{"cell_type":"markdown","source":"### Correlacion con la variable target","metadata":{}},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(15, 55)})\nsns.barplot(x=df_train.corr()[\"target\"].sort_values(ascending=False)[1: -1].values, y=df_train.corr()[\"target\"].sort_values(ascending=False)[1: -1].index, palette=\"RdBu\")\nplt.title(\"Correlacion de features con la variable target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:07:20.467546Z","iopub.execute_input":"2022-08-17T22:07:20.467945Z","iopub.status.idle":"2022-08-17T22:08:05.587114Z","shell.execute_reply.started":"2022-08-17T22:07:20.467911Z","shell.execute_reply":"2022-08-17T22:08:05.585894Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Como vemos hay variables muy correlacionadas positiva y negativamente y es muy probable que sean utilizadas por el modelo ya que son las que mas cantidad de informacion va a darnos para nuestras predicciones","metadata":{}},{"cell_type":"markdown","source":"### Normalización de los datos\n\nNo consideramos necesario realizarla ya que en la información que nos brinda la competencia se menciona que las features ya están normalizadas.","metadata":{}},{"cell_type":"markdown","source":"### Sobre las variables categoricas","metadata":{}},{"cell_type":"markdown","source":"Como vimos tenemos variables categoricas de las cuales queremos ver si podemos sacar mas informacion o correlacion","metadata":{}},{"cell_type":"code","source":"categoricas = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_68']","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:05.588560Z","iopub.execute_input":"2022-08-17T22:08:05.589004Z","iopub.status.idle":"2022-08-17T22:08:05.594741Z","shell.execute_reply.started":"2022-08-17T22:08:05.588960Z","shell.execute_reply":"2022-08-17T22:08:05.593671Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 16))\nfor index, feature in enumerate(categoricas):\n    plt.subplot(4, 3, index+1)\n    df_temp = pd.DataFrame(df_train[feature][df_train.target == 0].value_counts(normalize=True).sort_index().rename('count'))\n    plt.bar(df_temp.index, df_temp['count'], alpha=0.5, label='target=0')\n    df_temp = pd.DataFrame(df_train[feature][df_train.target == 1].value_counts(normalize=True).sort_index().rename('count'))\n    plt.bar(df_temp.index, df_temp['count'], alpha=0.5, label='target=1')\n    plt.xlabel(feature)\n    plt.ylabel('frecuencia')\n    plt.legend()\nplt.suptitle('Frecuencia del target en features categoricas', fontsize=20, y=0.93)\nplt.show()\ndel df_temp","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:05.595889Z","iopub.execute_input":"2022-08-17T22:08:05.596264Z","iopub.status.idle":"2022-08-17T22:08:07.572584Z","shell.execute_reply.started":"2022-08-17T22:08:05.596232Z","shell.execute_reply":"2022-08-17T22:08:07.571548Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Esto sirve como primer analisis general en las variables categoricas como vemos algunas features tienen una frecuencia mayor del target 1 que el target 0 y probablemente sea utilizada por nuestros modelos para la prediccion","metadata":{}},{"cell_type":"markdown","source":"### Sobre los días de la semana\n\nBasandonos en lo que observamos en una de las notebooks de la competencia, concluímos que podía ser interesante evaluar si hay alguna correlación entre el target y los días de la semana de S_2","metadata":{}},{"cell_type":"code","source":"!pip install july\nimport datetime\nimport july\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-17T22:08:07.573956Z","iopub.execute_input":"2022-08-17T22:08:07.574475Z","iopub.status.idle":"2022-08-17T22:08:20.690847Z","shell.execute_reply.started":"2022-08-17T22:08:07.574436Z","shell.execute_reply":"2022-08-17T22:08:20.689628Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train = df_train.copy()\nlabels = df_sample_casos.copy()\n\n\ntrain['S_2'] = train['S_2'].apply(datetime.datetime.fromisoformat)\ntrain['N'] = train.groupby('customer_ID')['customer_ID'].transform('count')\ntrain = train.loc[train.N==13] \n\ntx = train.groupby('S_2')['customer_ID'].count().reset_index(name='N')\nty = train.groupby('S_2')['target'].mean().reset_index(name='target_rate')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:20.692460Z","iopub.execute_input":"2022-08-17T22:08:20.692843Z","iopub.status.idle":"2022-08-17T22:08:21.423762Z","shell.execute_reply.started":"2022-08-17T22:08:20.692805Z","shell.execute_reply":"2022-08-17T22:08:21.422618Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"july.heatmap(tx['S_2'], tx['N'], title='train count', cmap=\"golden\", colorbar=True, month_grid=True)\njuly.heatmap(ty['S_2'], ty['target_rate'], title='target rate', cmap=\"golden\", colorbar=True, month_grid=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:21.425191Z","iopub.execute_input":"2022-08-17T22:08:21.425661Z","iopub.status.idle":"2022-08-17T22:08:22.139164Z","shell.execute_reply.started":"2022-08-17T22:08:21.425614Z","shell.execute_reply":"2022-08-17T22:08:22.138330Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"suma_correlacion_por_dia_de_la_semana = [0, 0, 0, 0, 0, 0, 0]\nocurrencias_por_dia_de_la_semana = [0, 0, 0, 0, 0, 0, 0]\n\nfor index, row in ty.iterrows():\n    suma_correlacion_por_dia_de_la_semana[row['S_2'].weekday()] += row['target_rate']\n    ocurrencias_por_dia_de_la_semana[row['S_2'].weekday()] += 1\n\ncorrelacion_por_dia_de_la_semana = [m/n for m, n in zip(suma_correlacion_por_dia_de_la_semana, ocurrencias_por_dia_de_la_semana)]\ncorrelacion_por_dia_de_la_semana","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:22.140181Z","iopub.execute_input":"2022-08-17T22:08:22.140951Z","iopub.status.idle":"2022-08-17T22:08:22.178350Z","shell.execute_reply.started":"2022-08-17T22:08:22.140891Z","shell.execute_reply":"2022-08-17T22:08:22.177220Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Observamos que algunos días (por ejemplo los domingos) muestran una correlación minimamente superior a las de otros, pero la correlacion es muy baja para todos respecto a esta variable. Es posible que se termine eliminando ya que el analisis nos dice que no es relevante para la prediccion del target.","metadata":{}},{"cell_type":"markdown","source":"## Transformación","metadata":{}},{"cell_type":"markdown","source":"Para empezar con la transformacion de nuestros datos vamos a eliminar en un principio las variables que tengan una media de valores NaNs mayor al 70%.","metadata":{}},{"cell_type":"code","source":"df_train_reduced = df_train.loc[:, df_train.isnull().mean() < 0.7].copy()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:22.179725Z","iopub.execute_input":"2022-08-17T22:08:22.180190Z","iopub.status.idle":"2022-08-17T22:08:22.643734Z","shell.execute_reply.started":"2022-08-17T22:08:22.180149Z","shell.execute_reply":"2022-08-17T22:08:22.642632Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Ahora deifinimos nuestras nuevas variables categoricas ya que se eliminaron algunas con gran cantidad de valores faltantes y chequeamos que valores tiene cada una para determinar una manera de rellenarlas.","metadata":{}},{"cell_type":"code","source":"for variable in categoricas:\n    print(df_train_reduced[variable].value_counts(dropna = False))","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:22.644984Z","iopub.execute_input":"2022-08-17T22:08:22.645337Z","iopub.status.idle":"2022-08-17T22:08:22.723340Z","shell.execute_reply.started":"2022-08-17T22:08:22.645307Z","shell.execute_reply":"2022-08-17T22:08:22.722354Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Vamos a rellenar todas las variables que no sean categoricas con la mediana de todos los valores de cada features. Por otro lado en el caso de la variable D_63 vemos que corresponde a prefijos de paises por lo tanto creamos una nueva categoria que sea \"No country\" que simbolice que no se tiene el pais de ese cliente. Por otro lado vemos que la feature \"D_64\" tiene un valor -1 que quizas representa valor faltante o algo de lo que no se tiene data asi que vamos a utilizar ese valor para completar. Por otro lado como dijimos anteriormente el resto de features que contengan el prefijo \"D\" les asignaremos un valor 0, ya que el NaN podria significar que no contienen registro de delito y por lo tanto no se tiene ese campo lo que seria igual a tener un 0. Por ultimo las demas features seran rellenadas con un valor extra en este caso el 9, esto funcionaria como una categoria extra representando que no existe ese valor, como no sabemos que representan las variables no podemos asignar un valor claro y representativo.","metadata":{}},{"cell_type":"code","source":"for column in df_train_reduced:\n    if column not in categoricas and column not in ignorar:\n        df_train_reduced[column].fillna(inplace = True, value = df_train_reduced[column].median())\n    elif column == \"D_63\":\n        df_train_reduced[column].fillna(inplace = True, value = \"No country\")\n    elif column == \"D_64\":\n        df_train_reduced[column].fillna(inplace = True, value = '-1')\n    elif column.startswith(\"D\"):\n        df_train_reduced[column].fillna(inplace = True, value = 0)\n    else:\n        df_train_reduced[column].fillna(inplace = True, value = 9)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:22.724681Z","iopub.execute_input":"2022-08-17T22:08:22.725054Z","iopub.status.idle":"2022-08-17T22:08:23.696200Z","shell.execute_reply.started":"2022-08-17T22:08:22.725002Z","shell.execute_reply":"2022-08-17T22:08:23.695080Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"A continuacion nosotros vamos a querer una sola prediccion por cada cliente, cada uno representado por su ID. Los datos previstos hacen referencia a muchas \"transacciones\" o muchos \"casos\" para cada cliente asi que vamos a calcular la media (en caso de variables numericas) y la moda (en caso de variables categoricas) entre todos los casos que tiene un mismo cliente, y nos quedaremos con esos valores para representarlo. De esta manera nos quedamos con una representacion por cada cliente y podemos hacer nuestras predicciones. Nos quedamos con la cantidad de veces que estaba repetido para crear una nueva feature con esta caracteristica, quizas sea util para nuestro entrenamiento.","metadata":{}},{"cell_type":"code","source":"repeticiones = df_train_reduced['customer_ID'].value_counts().sort_index()\nrepeticiones","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:23.697666Z","iopub.execute_input":"2022-08-17T22:08:23.698066Z","iopub.status.idle":"2022-08-17T22:08:23.753870Z","shell.execute_reply.started":"2022-08-17T22:08:23.698006Z","shell.execute_reply":"2022-08-17T22:08:23.752887Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"categoricas_and_customer = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_68', 'customer_ID']\ndf_train_cat = df_train_reduced[categoricas_and_customer].copy()\ndf_train_num = df_train_reduced.drop(columns = categoricas).copy()\n\ndf_train_num = df_train_num.groupby(\"customer_ID\").mean()\ndf_train_cat = df_train_cat.groupby(\"customer_ID\").agg(lambda x: pd.Series.mode(x)[0])","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:23.755251Z","iopub.execute_input":"2022-08-17T22:08:23.756352Z","iopub.status.idle":"2022-08-17T22:08:55.494105Z","shell.execute_reply.started":"2022-08-17T22:08:23.756312Z","shell.execute_reply":"2022-08-17T22:08:55.493094Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df_train_final = pd.merge(df_train_num, df_train_cat, left_on='customer_ID', right_on='customer_ID')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:55.495526Z","iopub.execute_input":"2022-08-17T22:08:55.496070Z","iopub.status.idle":"2022-08-17T22:08:55.555671Z","shell.execute_reply.started":"2022-08-17T22:08:55.496006Z","shell.execute_reply":"2022-08-17T22:08:55.554568Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df_train_final = df_train_final.reset_index()\ndf_train_final[\"customer_ID\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:55.557124Z","iopub.execute_input":"2022-08-17T22:08:55.557506Z","iopub.status.idle":"2022-08-17T22:08:55.609087Z","shell.execute_reply.started":"2022-08-17T22:08:55.557472Z","shell.execute_reply":"2022-08-17T22:08:55.607935Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df_train_final.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:55.610466Z","iopub.execute_input":"2022-08-17T22:08:55.611024Z","iopub.status.idle":"2022-08-17T22:08:55.643398Z","shell.execute_reply.started":"2022-08-17T22:08:55.610983Z","shell.execute_reply":"2022-08-17T22:08:55.642129Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"df_train_final['repeticiones'] = repeticiones.values","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:55.645270Z","iopub.execute_input":"2022-08-17T22:08:55.646328Z","iopub.status.idle":"2022-08-17T22:08:55.652287Z","shell.execute_reply.started":"2022-08-17T22:08:55.646279Z","shell.execute_reply":"2022-08-17T22:08:55.651384Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"Vamos a realizar un pequeño análisis para ver si valdría la pena quedarnos con la información de cuantas repeticiones tenía cada ID","metadata":{}},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(15, 7)})\nsns.countplot(data=df_train_final, y=\"repeticiones\" , hue=\"target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:55.655759Z","iopub.execute_input":"2022-08-17T22:08:55.656717Z","iopub.status.idle":"2022-08-17T22:08:56.050953Z","shell.execute_reply.started":"2022-08-17T22:08:55.656679Z","shell.execute_reply":"2022-08-17T22:08:56.049967Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar, la relación entre casos de default y no default no es la misma para cada cantidad de repeticiones, quizas a los modelos les sirve este feature.","metadata":{}},{"cell_type":"markdown","source":"## Conclusiones del analisis","metadata":{}},{"cell_type":"markdown","source":"Con todo lo analizado podemos acercarnos a una conclusion acerca de los datos previstos, si bien es complicado sacar conclusiones de las features ya que no tienen una representacion que nos permita ver que significan en la realidad, por eso vamos a realizar algunas suposiciones para poder analizar los datos de una manera mas clara:\n- Las \"features de Delincuencia\" (Delinquency variables) tienen un alto contenido de nulos, de hecho 24 de las 30 variables con mas nulos pertenecen a esta categoria. Asumiendo que refieren a delitos o alguna falta en transacciones con tarjeta podemos decir que los nulos probablemente sean casos en los que las persona no tiene faltas por lo tanto pueden ser variables interesantes en los casos en que la persona tenga alguna falta o sea no pertenezca a ese valor 0. Como vemos en la visualizacion hay variables de esta categoria altamente correlacionadas con nuestra variable target.\n- Sabiendo que la variable **P_2** es una de las mas correlacionadas (negativamente) es posible que esta feature perteneciente a la categoria de pagos se relacione con el plazo en el cual se paga la tarjeta y en caso de siempre cumplir con la regla de pagarla a tiempo es altamente probable que la persona no vaya a entrar en este \"Default\".\n- Algunas features presentan outliers un poco severos y que se van de los valores tipicos de las otras entradas. Sabiendo que el target con clase minoritaria es el que buscamos, suponemos que esos outliers son casos fuera de la norma que pueden conducir a un Default en un futuro ya que presentan irregularidades.\n- Una suposicion que quizas tendria sentido hacer es que la presencia de valores nulos corresponda a personas que tienen una cuenta creada hace relativamente poco tiempo, quizas todavia no son calculados los valores para las diferentes features cuando todavia no paso un tiempo considerable de uso, esto refuerza reemplazar mucho de los NaNs por un valor 0 o un numero representando otra categoria distinta\n- Muchas variables parecen no ser relevantes para determinar el Default de un cliente. Probablemente sean descartadas para el entrenamiento del modelo y necesitemos de menos para obtener la misma cantidad de informacion, este dataset debe ser una recopilacion entera de datos especificos de clientes sin necesidad de relacionarse con el Default.\n- No vimos una relevancia grande en las fechas de cada entrada. Es posible que las fechas simplemente sean una fecha por cada transaccion o accion que emite el cliente y no tengan especial relevancia para un suceso en particular, es un movimiento mas en esa cuenta.","metadata":{}},{"cell_type":"markdown","source":"### Metricas a utilizar\n\nNos decidimos por f1 y ROC-AUC","metadata":{}},{"cell_type":"markdown","source":"# Reduccion de dimensionalidad (PCA)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T04:14:48.316881Z","iopub.execute_input":"2022-08-08T04:14:48.317782Z","iopub.status.idle":"2022-08-08T04:14:54.049561Z","shell.execute_reply.started":"2022-08-08T04:14:48.317700Z","shell.execute_reply":"2022-08-08T04:14:54.048459Z"}}},{"cell_type":"markdown","source":"Como nuestros datos tienen muchisimas features y eso va a imposibilitar el entrenamiento de los modelos en un tiempo razonable vamos a aplicar una reduccion de dimensionalidad.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:56.052379Z","iopub.execute_input":"2022-08-17T22:08:56.053171Z","iopub.status.idle":"2022-08-17T22:08:56.246626Z","shell.execute_reply.started":"2022-08-17T22:08:56.053124Z","shell.execute_reply":"2022-08-17T22:08:56.245393Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"df_train = df_train_final.drop(columns = [\"customer_ID\", \"target\"])\npd.get_dummies(data = df_train, columns = [\"D_63\", \"D_64\"])\ndf_train.drop(columns = [\"D_63\", \"D_64\"], inplace = True)\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:56.248085Z","iopub.execute_input":"2022-08-17T22:08:56.249369Z","iopub.status.idle":"2022-08-17T22:08:56.340699Z","shell.execute_reply.started":"2022-08-17T22:08:56.249318Z","shell.execute_reply":"2022-08-17T22:08:56.339743Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"pca = PCA(svd_solver = \"randomized\", random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:56.343975Z","iopub.execute_input":"2022-08-17T22:08:56.344772Z","iopub.status.idle":"2022-08-17T22:08:56.349054Z","shell.execute_reply.started":"2022-08-17T22:08:56.344735Z","shell.execute_reply":"2022-08-17T22:08:56.348087Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pca.fit(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:56.350531Z","iopub.execute_input":"2022-08-17T22:08:56.350877Z","iopub.status.idle":"2022-08-17T22:08:57.812128Z","shell.execute_reply.started":"2022-08-17T22:08:56.350845Z","shell.execute_reply":"2022-08-17T22:08:57.811013Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"pca.components_","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:57.813683Z","iopub.execute_input":"2022-08-17T22:08:57.814183Z","iopub.status.idle":"2022-08-17T22:08:57.830510Z","shell.execute_reply.started":"2022-08-17T22:08:57.814139Z","shell.execute_reply":"2022-08-17T22:08:57.829504Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nvar_cumu = np.cumsum(pca.explained_variance_ratio_) * 100\nk = np.argmax(var_cumu > 95)\nplt.plot(var_cumu)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(x=k, color=\"k\", linestyle=\"--\")\nplt.axhline(y=95, color=\"r\", linestyle=\"--\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:57.835734Z","iopub.execute_input":"2022-08-17T22:08:57.838535Z","iopub.status.idle":"2022-08-17T22:08:58.135686Z","shell.execute_reply.started":"2022-08-17T22:08:57.838478Z","shell.execute_reply":"2022-08-17T22:08:58.134747Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nplt.plot(pca.explained_variance_ratio_[0:30],'o-',markersize=2)\nplt.xlabel('Componente')\nplt.ylabel('Varianza explicada')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:58.137176Z","iopub.execute_input":"2022-08-17T22:08:58.137852Z","iopub.status.idle":"2022-08-17T22:08:58.402021Z","shell.execute_reply.started":"2022-08-17T22:08:58.137807Z","shell.execute_reply":"2022-08-17T22:08:58.401009Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Con los datos calculados y las visualizaciones hechas podemos ver que con 20 componentes de PCA ya tenemos el 95% de la varianza explicada por lo tanto podemos aplicar PCA con 20 componentes y contendria la mayoria de la informacion que nuestros datos nos estan diciendo","metadata":{}},{"cell_type":"code","source":"final_pca = PCA(n_components = 20)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:58.403633Z","iopub.execute_input":"2022-08-17T22:08:58.404413Z","iopub.status.idle":"2022-08-17T22:08:58.409619Z","shell.execute_reply.started":"2022-08-17T22:08:58.404363Z","shell.execute_reply":"2022-08-17T22:08:58.408459Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"df_pca_transform = pd.DataFrame(final_pca.fit_transform(df_train))","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:58.411211Z","iopub.execute_input":"2022-08-17T22:08:58.412202Z","iopub.status.idle":"2022-08-17T22:08:58.772133Z","shell.execute_reply.started":"2022-08-17T22:08:58.412154Z","shell.execute_reply":"2022-08-17T22:08:58.771050Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"df_pca_transform[\"target\"] = df_train_final[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:58.773659Z","iopub.execute_input":"2022-08-17T22:08:58.774399Z","iopub.status.idle":"2022-08-17T22:08:58.782861Z","shell.execute_reply.started":"2022-08-17T22:08:58.774350Z","shell.execute_reply":"2022-08-17T22:08:58.781875Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df_pca_transform.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:58.784383Z","iopub.execute_input":"2022-08-17T22:08:58.786062Z","iopub.status.idle":"2022-08-17T22:08:58.851454Z","shell.execute_reply.started":"2022-08-17T22:08:58.785989Z","shell.execute_reply":"2022-08-17T22:08:58.850469Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"df_pca_transform.to_csv('df_train_pca.csv', sep=\";\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T22:08:58.856571Z","iopub.execute_input":"2022-08-17T22:08:58.859301Z","iopub.status.idle":"2022-08-17T22:08:59.581417Z","shell.execute_reply.started":"2022-08-17T22:08:58.859249Z","shell.execute_reply":"2022-08-17T22:08:59.580308Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Modelos\n\nNotebook con los modelos: https://www.kaggle.com/code/ignacioavecilla/tp3-datos-modelos","metadata":{}},{"cell_type":"markdown","source":"## Conclusiones finales\n\nLos modelos con los que mejores resultados obtuvimos fueron la red neuronal y el ensamble en cascada, ambos con resultados bastante similares. Si lo que se quiere es aumentar la precisión con la que se detectan los casos de default, probablemente el ensamble sea la mejor opción. El problema es que quedan muchos casos sin clasificar, casi un 40%, que en este caso decidimos volver a pasarselos a la red ya que es nuestro mejor modelo.\n\nConcluímos que es posible detectar si un cliente va a dejar de pagar en la mayoría de los casos, pero teniendo tan poca información sobre el significado de los datos es complejo realizar el tratamiento necesario de los mismos para obtener el mejor resultado posible.  \n\nInformación como antecendentes penales o historial crediticio podrían ser muy útiles para lograr mejores predicciones.\n\nSi se tuviera conocimiento más certero sobre el significado de los datos, probablemente la intervención humana tendría un peso relevante en un sistema en producción, pudiendo investigar minusiosamente casos particulares. ","metadata":{}}]}